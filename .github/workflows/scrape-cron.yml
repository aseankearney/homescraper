name: Scrape Listings

on:
  schedule:
    - cron: "0 18 * * *" # 10 AM PT
    - cron: "0 22 * * *" # 2 PM PT
  workflow_dispatch: {}

jobs:
  scrape-and-write-sheet:
    runs-on: ubuntu-latest
    steps:
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests google-auth google-api-python-client

      - name: Scrape Craigslist and append to Google Sheet
        env:
          GOOGLE_SERVICE_ACCOUNT_JSON: ${{ secrets.GOOGLE_SERVICE_ACCOUNT_JSON }}
          GOOGLE_SHEET_ID: ${{ secrets.GOOGLE_SHEET_ID }}
        run: |
          python <<'PY'
          import json
          import re
          import time
          import random
          import requests
          import xml.etree.ElementTree as ET
          from datetime import datetime
          from google.oauth2 import service_account
          from googleapiclient.discovery import build

          CONFIG = {
            "minPrice": 2000,
            "maxPrice": 2700,
            "minBedrooms": 1,
            "locations": [
              "Woodland Hills",
              "West Hills",
              "Newbury Park",
              "Calabasas",
              "Sherman Oaks",
              "Thousand Oaks",
              "Oak Park",
              "Simi Valley",
            ],
          }

          CITY_ALIASES = {
            "canoga park": "West Hills",
            "winnetka": "West Hills",
            "reseda": "Woodland Hills",
            "encino": "Woodland Hills",
            "northridge": "Woodland Hills",
          }

          NO_PET_MARKERS = ["no pets", "no pet", "pets not allowed", "pet free", "no dogs", "no cats"]

          def normalize_city(city: str) -> str:
            lower = (city or "").lower().strip()
            return CITY_ALIASES.get(lower, city)

          def infer_property_type(title: str, description: str) -> str:
            s = f"{title} {description}".lower()
            if "townhouse" in s or "town house" in s or "townhome" in s:
              return "townhouse"
            if "condo" in s or "condominium" in s:
              return "condo"
            if "apartment" in s or " apt " in f" {s} " or "studio" in s:
              return "apartment"
            if "adu" in s or "granny flat" in s or "guest house" in s or "in-law" in s:
              return "adu"
            if "house" in s or "single-family" in s or "sfr" in s:
              return "house"
            return "other"

          def category_bucket(property_type: str) -> str:
            if property_type in ("house", "adu"):
              return "House"
            if property_type == "apartment":
              return "Apartment"
            if property_type in ("condo", "townhouse"):
              return "Condo/Townhouse"
            return "Other/Unknown"

          def is_no_pets(text: str) -> bool:
            lower = (text or "").lower()
            return any(marker in lower for marker in NO_PET_MARKERS)

          def fetch_rss_with_retries(city_query: str) -> str:
            q = requests.utils.quote(city_query)
            urls = [
              f"https://losangeles.craigslist.org/search/apa?format=rss&query={q}&min_price={CONFIG['minPrice']}&max_price={CONFIG['maxPrice']}&min_bedrooms={CONFIG['minBedrooms']}",
              f"https://losangeles.craigslist.org/search/apa?query={q}&min_price={CONFIG['minPrice']}&max_price={CONFIG['maxPrice']}&min_bedrooms={CONFIG['minBedrooms']}&format=rss&sort=date",
              f"https://losangeles.craigslist.org/search/apa?query={q}&format=rss",
            ]
            user_agents = [
              "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_6_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
              "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
              "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            ]
            last = "unknown"
            for url in urls:
              for attempt in range(1, 4):
                try:
                  r = requests.get(url, headers={
                    "User-Agent": user_agents[(attempt - 1) % len(user_agents)],
                    "Accept": "application/rss+xml, application/xml;q=0.9, */*;q=0.8",
                    "Referer": "https://losangeles.craigslist.org/",
                  }, timeout=15)
                  if r.status_code == 200:
                    return r.text
                  last = f"HTTP {r.status_code}"
                  if r.status_code in (403, 429) or r.status_code >= 500:
                    time.sleep(0.4 * (2 ** (attempt - 1)) + random.random() * 0.3)
                    continue
                  r.raise_for_status()
                except Exception as e:
                  last = str(e)
                  time.sleep(0.4 * (2 ** (attempt - 1)) + random.random() * 0.3)
            raise RuntimeError(last)

          def parse_city(city_query: str):
            xml_text = fetch_rss_with_retries(city_query)
            root = ET.fromstring(xml_text)
            items = root.findall("./channel/item")
            out = []

            for item in items:
              title = item.findtext("title", default="")
              url = item.findtext("link", default="")
              description = item.findtext("description", default="")
              timestamp = item.findtext("{http://purl.org/dc/elements/1.1/}date", default=datetime.utcnow().isoformat())
              if not title or not url:
                continue
              if is_no_pets(title) or is_no_pets(description):
                continue

              m = re.search(r"\$(\d+)", title)
              price = int(m.group(1)) if m else 0
              if price and (price < CONFIG["minPrice"] or price > CONFIG["maxPrice"]):
                continue

              b = re.search(r"(\d+)br", title, flags=re.I)
              bedrooms = int(b.group(1)) if b else 1
              if bedrooms < CONFIG["minBedrooms"]:
                continue

              city = city_query
              lower_title = title.lower()
              for loc in CONFIG["locations"]:
                if loc.lower() in lower_title:
                  city = loc
                  break
              city = normalize_city(city)

              property_type = infer_property_type(title, description)
              bucket = category_bucket(property_type)

              id_match = re.search(r"/(\d+)\.html", url)
              listing_id = f"craigslist:{id_match.group(1) if id_match else str(int(time.time() * 1000))}"
              address = "-".join(title.split("-")[1:]).strip() or title

              out.append([
                listing_id,
                "craigslist",
                timestamp,
                city,
                price,
                bedrooms,
                property_type,
                bucket,
                "new",
                address,
                url,
              ])

            return out

          raw = os.environ.get("GOOGLE_SERVICE_ACCOUNT_JSON")
          sheet_id = os.environ.get("GOOGLE_SHEET_ID")
          if not raw:
            raise RuntimeError("Missing GOOGLE_SERVICE_ACCOUNT_JSON")
          if not sheet_id:
            raise RuntimeError("Missing GOOGLE_SHEET_ID")

          credentials = service_account.Credentials.from_service_account_info(
            json.loads(raw),
            scopes=[
              "https://www.googleapis.com/auth/spreadsheets",
              "https://www.googleapis.com/auth/drive",
            ],
          )
          sheets = build("sheets", "v4", credentials=credentials)

          existing = sheets.spreadsheets().values().get(
            spreadsheetId=sheet_id,
            range="Sheet1!A2:A",
          ).execute().get("values", [])
          existing_ids = {row[0] for row in existing if row}

          all_rows = []
          errors = []
          for city in CONFIG["locations"]:
            try:
              all_rows.extend(parse_city(city))
            except Exception as e:
              errors.append(f"{city}: {e}")

          deduped = []
          seen = set()
          for row in all_rows:
            listing_id = row[0]
            if listing_id in seen or listing_id in existing_ids:
              continue
            seen.add(listing_id)
            deduped.append(row)

          if deduped:
            sheets.spreadsheets().values().append(
              spreadsheetId=sheet_id,
              range="Sheet1!A:K",
              valueInputOption="RAW",
              body={"values": deduped},
            ).execute()

          print(json.dumps({
            "totalFetched": len(all_rows),
            "newCount": len(deduped),
            "errorCount": len(errors),
            "errors": errors,
          }, indent=2))

          if len(all_rows) == 0 and len(errors) > 0:
            print("Craigslist blocked this run; preserved existing sheet data.")
          PY
